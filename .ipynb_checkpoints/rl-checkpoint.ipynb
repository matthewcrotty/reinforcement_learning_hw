{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "One real world problem that can be framed as a MDP is the optimization of traffic light patterns to reduce the time that cars are stopped at red lights. In high school, I worked on a group project that aimed to write a similation or a model that could optimize traffic patterns through a decentralized network that passed information from intersection to intersection, in an attempt to turn \"Smart\" traffic lights, which are just stops with sensors, to predictive smart lights, which would know in advance how much incoming traffic there was in which directions. However, our project was truncated to just a simulation that ran traffic light intersections and patterns and allowed calculation of statistics, which could in the future be used to train some model that achived the original goal of the project. One of the reasons for this was the lack of ML or AI knowledge and techniques that we knew as high schoolers. \n",
    "\n",
    "But now looking back on the project, there is a possibility that we could have used MDP in some effect to test and train traffic pattern policies. The states of the model would be the currect color of the traffic lights, the current time of day and the amount of cars at each light. The action space is just advancing each of the lights to the next color, red->yellow->green. The transition space is how the cars flow to the next intersection, with some probability that cars are joining and leaving the road inbetween the stop lights, and an increase in time of day. The reward space is how much each action would reduce the overall time cars spend at red lights. It is important to consider the time of day due to regular traffic patterns like rush hour. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "The field I looked into for applications of reinforcement learning was automated stock trading. The general idea is to have some algorithm that can automatically buy and sell stocks at the right times to maximize profit. The model is trained using past stock data which usually includes price, volume, and other factors such as strength and resiliance measures. The reward used is just when then model makes trades that increase the profit overall, there is more reward.\n",
    "\n",
    "One open-source project I found that implements reinforcement learning to trade stocks was \"Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy,\" a paper submitted to ICAIF, a confrence on AI in Finance. The github for the code can be found at: https://github.com/AI4Finance-Foundation/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020.  The project first starts with 7 factors that make up the data: Availible balance, closing price, shares owned, Moving Average Convergence Divergence, Relative Strength Index, Commodity Channel Index, and the Average Directional Index. The action space they consider is how much of each stock to buy and sell at each time. The authors then implement an ensemble strategy to create the model. The three algorithms they use are Advantage Actor Critic (A2C), Deep Deterministic Policy Gradient (DDPG), and Proximal Policy Optimization (PPO). A2C looks to optimize the policy gradient of the model by looking at the ways in which the policy gradient can improve, not just in the ways it can be less worse. DDPG uses neural networks to aid the policy gradient approximations, and using Q-learning to optimize. PPO improves the stability of the model by ensuring not too much change happens overtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "board = np.zeros((3, 3))\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def valid_moves(board):\n",
    "    return np.argwhere(board == 0)\n",
    "\n",
    "# Returns 1 if player 1 wins, -1 if player -1, else 0\n",
    "def game_over(board):\n",
    "    if len(valid_moves(board)) == 0:\n",
    "        return 1\n",
    "    for row in board:\n",
    "        if len(set(row)) == 1:\n",
    "            return row[0]\n",
    "    for col in np.transpose(board):\n",
    "        if len(set(col)) == 1:\n",
    "            return col[0]\n",
    "    if len(set([board[0][0], board[1][1], board[2][2]])) == 1:\n",
    "           return board[1][1]\n",
    "    if len(set([board[0][2], board[1][1], board[2][0]])) == 1:\n",
    "           return board[1][1]\n",
    "    return 0\n",
    "\n",
    "def random_agent(board):\n",
    "    moves = valid_moves(board)\n",
    "    m = random.randrange(0, len(moves))\n",
    "    return moves[m]\n",
    "\n",
    "\n",
    "board = np.array([[1, 1, -1],\n",
    "                  [0, 0, 0],\n",
    "                  [0, 0, -1]])\n",
    "random_agent(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_agent():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game loop\n",
    "for t in range(100):\n",
    "    board = np.zeros((3,3))\n",
    "    while game_over(board) == 0:\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
